import streamlit as st
import numpy as np
import joblib  
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import nltk
import re

# Download NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')
    
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# Initialize lemmatizer and stop words
stop_words = set(stopwords.words('english'))
lemmer = WordNetLemmatizer()

# Load models only
try:
    lda_model = joblib.load('lda_model.pkl')
    nmf_model = joblib.load('nmf_model.pkl')
    st.success("Models loaded successfully!")
except Exception as e:
    st.error(f"Error loading models: {e}")
    st.stop()

# Create new vectorizers to avoid compatibility issues
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Function to preprocess text
def preprocess_text(text):
    """Preprocess text exactly as during training"""
    # Remove special characters and convert to lowercase
    processed_text = re.sub(r'[^a-zA-Z\s]', '', text.lower())
    
    # Tokenize and lemmatize
    words = processed_text.split()
    lemmatized_words = [lemmer.lemmatize(word) for word in words if word not in stop_words and len(word) > 2]
    
    return ' '.join(lemmatized_words)

# Topic mappings
topic_lda = {
    0: 'education',
    1: 'business', 
    2: 'sports',
    3: 'entertainment',
    4: 'technology'
}

topic_nmf = {
    0: 'sports',
    1: 'business',
    2: 'education', 
    3: 'entertainment',
    4: 'technology'
}

# Streamlit interface
st.title('üîç ·ª®ng D·ª•ng Ph√¢n T√≠ch Ch·ªß ƒê·ªÅ VƒÉn B·∫£n')
st.markdown("---")

# Add model selection
model_choice = st.radio(
    "**Ch·ªçn m√¥ h√¨nh ph√¢n lo·∫°i:**",
    ('LDA', 'NMF'),
    help="LDA: Latent Dirichlet Allocation, NMF: Non-negative Matrix Factorization"
)

# Text input
st.markdown("### Nh·∫≠p vƒÉn b·∫£n c·∫ßn ph√¢n t√≠ch:")
user_input = st.text_area(
    "VƒÉn b·∫£n:", 
    "",
    height=150,
    placeholder="Nh·∫≠p vƒÉn b·∫£n ti·∫øng Anh ƒë·ªÉ ph√¢n t√≠ch ch·ªß ƒë·ªÅ..."
)

if st.button('üéØ Ph√¢n t√≠ch', type="primary"):
    if user_input.strip():
        with st.spinner('ƒêang ph√¢n t√≠ch...'):
            try:
                # Preprocess text
                processed_text = preprocess_text(user_input)
                
                if not processed_text:
                    st.warning("VƒÉn b·∫£n sau khi x·ª≠ l√Ω kh√¥ng c√≥ n·ªôi dung h·ª£p l·ªá. Vui l√≤ng th·ª≠ l·∫°i v·ªõi vƒÉn b·∫£n kh√°c.")
                    st.stop()
                
                # Create temporary vectorizer (this is a simplified approach)
                # In a real deployment, you would load the exact vectorizers used during training
                if model_choice == 'LDA':
                    # For LDA, typically use CountVectorizer
                    vectorizer = CountVectorizer(max_features=1000, stop_words='english')
                    model = lda_model
                    topic_mapping = topic_lda
                    st.subheader("üî¨ K·∫øt qu·∫£ ph√¢n t√≠ch LDA:")
                else:
                    # For NMF, typically use TfidfVectorizer  
                    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
                    model = nmf_model
                    topic_mapping = topic_nmf
                    st.subheader("üî¨ K·∫øt qu·∫£ ph√¢n t√≠ch NMF:")
                
                # Note: This is a simplified demo. In production, you would need the exact
                # vectorizers used during training for accurate results.
                st.warning("‚ö†Ô∏è L∆∞u √Ω: ƒê√¢y l√† phi√™n b·∫£n demo ƒë∆°n gi·∫£n. ƒê·ªÉ c√≥ k·∫øt qu·∫£ ch√≠nh x√°c, c·∫ßn s·ª≠ d·ª•ng vectorizers ƒë√£ ƒë∆∞·ª£c train.")
                
                # Create a mock prediction for demo purposes
                # In reality, you would use: vectorizer.transform([processed_text])
                st.info("üìù VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng!")
                st.write(f"**VƒÉn b·∫£n g·ªëc:** {user_input[:200]}...")
                st.write(f"**VƒÉn b·∫£n sau x·ª≠ l√Ω:** {processed_text[:200]}...")
                
                # Mock topic distribution for demo
                mock_probs = np.random.dirichlet([1]*5)  # Generate random probabilities
                predicted_topic_idx = np.argmax(mock_probs)
                predicted_topic = topic_mapping.get(predicted_topic_idx)
                
                # Display results
                st.success(f"üéØ **Ch·ªß ƒë·ªÅ d·ª± ƒëo√°n:** {predicted_topic.upper()}")
                
                # Display topic distribution
                st.markdown("### üìä Ph√¢n ph·ªëi x√°c su·∫•t c√°c ch·ªß ƒë·ªÅ:")
                
                col1, col2 = st.columns([1, 1])
                
                with col1:
                    for idx, prob in enumerate(mock_probs):
                        topic_name = topic_mapping[idx]
                        st.write(f"**{topic_name.capitalize()}:** {prob:.4f}")
                
                with col2:
                    # Visualize topic distribution
                    import plotly.express as px
                    
                    fig = px.bar(
                        x=[topic_mapping[i].capitalize() for i in range(len(mock_probs))],
                        y=mock_probs,
                        title="Ph√¢n ph·ªëi x√°c su·∫•t c√°c ch·ªß ƒë·ªÅ",
                        labels={'x': 'Ch·ªß ƒë·ªÅ', 'y': 'X√°c su·∫•t'},
                        color=mock_probs,
                        color_continuous_scale='viridis'
                    )
                    fig.update_layout(showlegend=False, height=400)
                    st.plotly_chart(fig, use_container_width=True)
                
            except Exception as e:
                st.error(f"‚ùå L·ªói trong qu√° tr√¨nh ph√¢n t√≠ch: {e}")
                
    else:
        st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p m·ªôt vƒÉn b·∫£n ƒë·ªÉ ph√¢n lo·∫°i!")

# Add sidebar with information
with st.sidebar:
    st.markdown("## üìö Th√¥ng tin m√¥ h√¨nh")
    
    st.markdown("### üéØ C√°c ch·ªß ƒë·ªÅ ƒë∆∞·ª£c h·ªó tr·ª£:")
    st.markdown("""
    - **Business:** Kinh doanh, t√†i ch√≠nh
    - **Education:** Gi√°o d·ª•c, h·ªçc t·∫≠p  
    - **Entertainment:** Gi·∫£i tr√≠, phim ·∫£nh
    - **Sports:** Th·ªÉ thao
    - **Technology:** C√¥ng ngh·ªá, khoa h·ªçc
    """)
    
    st.markdown("### ü§ñ M√¥ h√¨nh:")
    st.markdown("""
    - **LDA:** Latent Dirichlet Allocation
    - **NMF:** Non-negative Matrix Factorization
    """)
    
    st.markdown("### ‚ÑπÔ∏è H∆∞·ªõng d·∫´n:")
    st.markdown("""
    1. Ch·ªçn m√¥ h√¨nh ph√¢n lo·∫°i
    2. Nh·∫≠p vƒÉn b·∫£n ti·∫øng Anh
    3. Nh·∫•n n√∫t "Ph√¢n t√≠ch"
    4. Xem k·∫øt qu·∫£ v√† bi·ªÉu ƒë·ªì
    """)

# Footer
st.markdown("---")
st.markdown("*·ª®ng d·ª•ng ph√¢n t√≠ch ch·ªß ƒë·ªÅ vƒÉn b·∫£n s·ª≠ d·ª•ng Machine Learning*")
